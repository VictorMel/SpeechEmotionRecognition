{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Packages\n",
    "!pip install kaggle --upgrade --quiet\n",
    "!pip install opendatasets --upgrade --quiet\n",
    "!pip install librosa --upgrade --quiet\n",
    "!pip install noisereduce --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import opendatasets as od\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import seaborn as sns\n",
    "import noisereduce as nr\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import wave\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import prjLib as lib\n",
    "import VictorLib as vic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "FRAME_LEN = 2048 # 4096\n",
    "HOP_LEN = 512 # 2048\n",
    "N = 128\n",
    "\n",
    "FIGX = 14\n",
    "FIGY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Logistic Regression\n",
    "oLR = LogisticRegression(random_state=0)\n",
    "oLR.fit(speech_data_features, speech_label_array)\n",
    "oLR.coef_.shape\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(oLR.coef_[0,:],label='neutral')\n",
    "plt.plot(oLR.coef_[1,:],label='calm')\n",
    "plt.plot(oLR.coef_[2,:],label='happy')\n",
    "plt.plot(oLR.coef_[3,:],label='sad')\n",
    "plt.plot(oLR.coef_[4,:],label='angry')\n",
    "plt.plot(oLR.coef_[5,:],label='fearful')\n",
    "plt.plot(oLR.coef_[6,:],label='disgust')\n",
    "plt.plot(oLR.coef_[7,:],label='surprised')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4060 Ti (UUID: GPU-9a58fd84-6033-0736-2543-7c922e05178a)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --list-gpus\n",
    "!conda create -n ml python=3.9\n",
    "# !conda activate ml\n",
    "# !conda install -c anaconda cudatoolkit\n",
    "!conda install -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.is_available() -> bool>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') # 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU speed\n",
      "0.682175874710083\n",
      "Verify device:  cuda:0\n",
      "GPU speed\n",
      "0.656245231628418\n",
      "Verify device:  cuda:0\n",
      "GPU speed\n",
      "0.6522750854492188\n",
      "Verify device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "matrix_size = 32*512\n",
    "x = torch.randn(matrix_size,matrix_size)\n",
    "y = torch.randn(matrix_size,matrix_size)\n",
    "'''\n",
    "print('CPU speed')\n",
    "start = time.time()\n",
    "result = torch.matmul(x,y)\n",
    "print(time.time()-start)\n",
    "print('Verify device: ',result.device)\n",
    "'''\n",
    "\n",
    "x_gpu = x.to(torch.device('cuda'))\n",
    "y_gpu = y.to(torch.device('cuda'))\n",
    "torch.cuda.synchronize()\n",
    "for ii in range(3):\n",
    "    print('GPU speed')\n",
    "    start = time.time()\n",
    "    result_gpu = x_gpu @ y_gpu # torch.matmul(x_gpu,y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    print(time.time()-start)\n",
    "    print('Verify device: ',result_gpu.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros([1,3],device=torch.device('cuda'))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2], dtype=torch.int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(np.array([1,2]))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 2*a\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install numba\n",
    "!conda install cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without GPU: 3.8525635870028054\n",
      "with GPU: 0.8630010680062696\n"
     ]
    }
   ],
   "source": [
    "from numba import jit, cuda \n",
    "import numpy as np \n",
    "# to measure exec time \n",
    "from timeit import default_timer as timer \n",
    "\n",
    "# normal function to run on cpu \n",
    "def func(a):\t\t\t\t\t\t\t\t \n",
    "\tfor i in range(10000000): \n",
    "\t\ta[i]+= 1\t\n",
    "\n",
    "# function optimized to run on gpu \n",
    "@jit(target_backend='cuda')\t\t\t\t\t\t \n",
    "def func2(a): \n",
    "\tfor i in range(10000000): \n",
    "\t\ta[i]+= 1\n",
    "if __name__==\"__main__\": \n",
    "\tn = 10000000\t\t\t\t\t\t\t\n",
    "\ta = np.ones(n, dtype = np.float64) \n",
    "\t\n",
    "\tstart = timer() \n",
    "\tfunc(a) \n",
    "\tprint(\"without GPU:\", timer()-start)\t \n",
    "\t\n",
    "\tstart = timer() \n",
    "\tfunc2(a) \n",
    "\tprint(\"with GPU:\", timer()-start) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

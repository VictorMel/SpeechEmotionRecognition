{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Packages\n",
    "!pip install kaggle --upgrade --quiet\n",
    "!pip install opendatasets --upgrade --quiet\n",
    "!pip install librosa --upgrade --quiet\n",
    "!pip install noisereduce --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import opendatasets as od\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import seaborn as sns\n",
    "import noisereduce as nr\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "import wave\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import prjLib as lib\n",
    "import VictorLib as vic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "FRAME_LEN = 2048 # 4096\n",
    "HOP_LEN = 512 # 2048\n",
    "N = 128\n",
    "\n",
    "FIGX = 14\n",
    "FIGY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_n_mfccs = np.arange(5,128)\n",
    "lScore = np.zeros([len(l_n_mfccs),2])\n",
    "for ii in tqdm(range(len(l_n_mfccs)), desc='Grid Searching model'):\n",
    "    lScore[ii,0] = DataPreprocessing(l_n_mfccs[ii])\n",
    "    lScore[ii,1] = l_n_mfccs[ii]\n",
    "best_results = SortDataDescent(lScore,0)\n",
    "best_score = best_results[0,0]\n",
    "best_n_mfcc = best_results[0,1]\n",
    "print(f'GridSearched: Best result for n_mfcc = {best_n_mfcc} with test score = {best_score}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeaturesGridSearch(frame_len,hop_len,N, plot):\n",
    "    # Collect Data and Extract Features from audio files\n",
    "    featured_data = []  # stores the features\n",
    "    labels = []  # stores the labels\n",
    "    for data_path in tqdm(data_paths,desc=\"Collecting Datasets\"):\n",
    "        root_dir = data_path[2]\n",
    "        cpt = sum([len(d) for r, d, files in os.walk(root_dir)])+1\n",
    "        for subdirs, dirs, files in tqdm(os.walk(root_dir), total=cpt, desc=f'Processing {data_path[0]} audio files', unit=\"file\"):\n",
    "            for file in files:\n",
    "                if not '.wav' in file:\n",
    "                    continue\n",
    "                audio_file_path = os.path.join(subdirs, file)\n",
    "                sample, sr = librosa.load(audio_file_path)\n",
    "                # fix_sample = FixSamples(sample,sr,4)\n",
    "                features = ExtractFeatures(sample,sr,frame_len,hop_len,N)\n",
    "                label = ExtractLabel(file,data_path[0])\n",
    "                featured_data.append(features)\n",
    "                labels.append(label)\n",
    "    featured_data = np.asarray(featured_data)\n",
    "    labels = np.array(labels)\n",
    "    print(f'Data with total {featured_data.shape[0]} samples collected with {featured_data.shape[1]} features.')\n",
    "\n",
    "    # Data samples split\n",
    "    train_data,train_labels,test_data,test_labels = lib.PlotSplitData(featured_data,labels,trainRatio=0.8,plot=plot)\n",
    "\n",
    "    '''\n",
    "    # Select features\n",
    "    delete_features_indexes = np.arange(0,0)\n",
    "    train_data = np.delete(train_data,delete_features_indexes,axis=1)\n",
    "    test_data = np.delete(test_data,delete_features_indexes,axis=1)\n",
    "    print(f'Selected {train_data.shape[1]} features.')\n",
    "    '''\n",
    "\n",
    "    # Data extrapulation of train data, for uniform histogram of classes\n",
    "    dataL1 = train_data[(train_labels==1)]\n",
    "    L1 = train_labels[(train_labels==1)]\n",
    "    dataL7 = train_data[(train_labels==7)]\n",
    "    L7 = train_labels[(train_labels==7)]\n",
    "    train_data = np.concatenate((train_data,dataL1,dataL1,dataL1,dataL1,dataL1,dataL7),axis=0)\n",
    "    train_labels = np.concatenate((train_labels,L1,L1,L1,L1,L1,L7),axis=0)\n",
    "    print(f'Train Data extrapulated with total {train_data.shape[0]} samples collected with {train_data.shape[1]} features.')\n",
    "\n",
    "    # Test features with a simple model\n",
    "    n_estimators = 100\n",
    "    min_samples_split = 6\n",
    "    random_state = 512\n",
    "    score, feature_importance = TestFeaturesInSimpleModel(train_data,train_labels,test_data,test_labels, n_estimators=n_estimators,min_samples_split=min_samples_split,random_state=random_state, plot=plot)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_len = 4096\n",
    "hop_len = 2048\n",
    "N = 14\n",
    "F = 10\n",
    "Z = 0.1\n",
    "lN = [14,20,40,128]\n",
    "bestScore = 0\n",
    "bestN = 0\n",
    "for N in tqdm(lN,desc=\"lN Grid Search\"):\n",
    "    score = FeaturesGridSearch(frame_len,hop_len,N,F,Z,plot=False)\n",
    "    if(score > bestScore):\n",
    "        bestScore = score\n",
    "        bestN = N\n",
    "print(f'Best score = {bestScore} of N = {bestN} .')\n",
    "FeaturesGridSearch(frame_len,hop_len,bestN,F,Z,plot=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
